# MPAS | 2025-01-29

## Reinforcement learning

- Q-Learning
  - Quality function
- Gittins index
- Deep Q-Networks
- Inverse reinforcement learning

## Multi-armed bandits

- Multi-armed bandit problem

- Exploration-Exploitation dilemma

### Applications

- Resource allocation

- Tree search

### Strategies

#### Follow the leader

- After $k$ time steps, just chose the best arm in that instance
- An elimination type strategy
  - ~Eliminate all the arms and just choose the best arm?

#### Explore then Commit

- Follow the leader, repeated $m$ times, and averaged

- Q: Does the order matter in which the $mk$ ~actions are chosen?

#### Epsilon greedy strategy

- $\varepsilon \rightarrow$ Explore

- $1 - \varepsilon \rightarrow$ Exploit

- Continuously updates the estimates

- Not an elimination type strategy

- $$
  \hat \mu_{i}(t) = \frac{1}{T_{i}(t)} \sum_{s = 1}^{s = t} \mathbb{I}_{A_s = i} X_s
  $$

  - Estimated reward function

#### Upper confidence bound

- Confidence bound interval

- $$
  \hat \mu_{i}(t) = \frac{1}{T_{i}(t)} \sum_{s = 1}^{s = t} \mathbb{I}_{A_s = i} X_s
  \\
  \operatorname{UCB}_{i}(t) = \hat \mu_{i}(t) + \sqrt{\frac{2 \log(t)}{T_{i}(t - 1)}}
  $$

- Over time, confidence increases

- Select maximum

#### Thompson sampling

- a.k.a. posterior sampling

- Beta distribution is generally assumed for Bernoulli bandits

  - $$
    \operatorname{Beta}(\alpha, \beta) = x^{\alpha - 1} {(1 - x)}^{\beta - 1}
    $$

  - ~Beta is the conjugate prior for Bernoulli

- Update $\alpha$ when you get a reward, and update $\beta$ when you don't

---

