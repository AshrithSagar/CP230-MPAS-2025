# MPAS | 2025-01-17

## Sequential decision problems

- Sequential decision problems vs. Episodic problems
- Environment is fully observant
- Actions are stochastic
- Markov decision process (MDP)
- Partially observable Markov decision process (POMDP)

- $T(s, a, s') \to$ State transition matrix
  - Deterministic actual values
    - In uninformed state space search, such as BFS, DFS, etc
  - Deterministic estimated values
    - In informed state space search, such as $\mathcal{A}^\ast, \mathcal{MA}^\ast$, etc
  - Probabilistic values
    - In Markov decision processes
  - Unkown
    - In reinforcement learning

## Markov decision process

- $S_0 \to$ Initial state
- $T(s, a, s') \to$ Transition model
  - Assumes Markovian property
- $R(s) \to$ Rewards associated with a state $s$. Short term reward for the state.
- Model $M_{ij}^a \equiv P(j \mid i, a) \to$ Probability that doing action $a$ leads state $i$ to $j$

## Policy

- A complete mapping from states to actions
- In search problems, the aim is to find an optimal sequence
- In MDP, the aim is to find an optimal policy $\pi^*$, because we can't predict where one will end up
- An optimal policy $\pi^*(s)$ yields maximum expected utility (MEU) of environment histories

---

