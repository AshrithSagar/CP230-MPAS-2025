# MPAS | 2025-01-24

## Reinforcement learning

- Superficial view on some algorithms

### Overview

- Passive reinforcement learning

  - Direct utility estimation
  - Adaptive dynamic programming

  - Temporal difference learning

- Active reinforcement learning
  - Q-learning
  - Multi-armed bandit problem

- Advanced reinforcement learning
  - Deep Q-learning
  - Inverse reinforcement learning

### Basics

- Agent
  - Can perceive and act on states
- Environment
  - States of the environment
- Rewards
  - Positive, Negative

- ~Goal state is not well known

- Learning theory

  - Supervised
  - Unsupervised

- Distinctions

  - Delayed reward

  - Agent choosed the training data

  - Explore vs Exploit dilemma

- Terminology
  - Reward $\to$ short term benefit
  - Utility $\to$ long term benefit
  - Policy
  - State-action mapping

### Setup

- MDP
- Set of states $s \in S$, the state space
- Set of actions $a \in A$, the action space; $A(s) \to$ actions available from state $s$
- Model $T(s, a, s')$
- Reward function $R(s, a, s')$
- Discount factor $\gamma$
- Looking for an (optimal) policy $\pi$
- Challenging part $\to$ don't know $T$ or $R$ in advance

---

- In value and policy iterations, we assume that we know the model. Here instead, we will learn the model.
- Markov property
  - Transition probabilities depends on the (current) state only and not on the path to the state
- Markov decision process (MDP)
- Partially observable Markov decision process (POMDP)

### Approaches

#### Model based approach

- Learn the model or an approximation of it

- Find optimal policy

- Learn the model empirically, through experience

  - Discover the rewards when experienced

- $$
  U_{i+1}^{\pi}(s) \leftarrow \sum_{s'} T(s, \pi(s), s') [R(s, \pi(s), s') + \gamma U_{i}^{\pi}(s')]
  $$

#### Model free approach

- Find optimal policy without explicitly learning the model
- Useful when model is complex or hard to represent

### Learning

#### Passive learning

- Fixed policy
- $\sim$ policy evaluation
- Often serves as a component to active learning
- Don't know the transition or the rewards, but given a policy $\pi$
  - Just execute the policy and learn on the run, from experience
  - Actually take actions and see what happens

#### Active learning

- Attempt to find an optimal
- $\sim$ solving the MDP, without being given the model

## Direct utility estimation

- a.k.a. Monte-Carlo approach

- Estimate $U^{\pi}(s)$

- Reward to go, of a state $s$, is the sum of the (discounted) rewards from $s$ till a terminal state

- Table of utilities
  - Turns into a supervised problem

- Utility values converge

  - Doens't expolit Bellman constraints

  - $$
    U^{\pi}(s) = R(s) + \beta \sum_{s'} T(s, \pi(s), s') U^{\pi}(s')
    $$

## Adaptive dynamic programming (ADP)

- A model-based approach

- Learning the state transition probability matrix

  - Learning is a problem, for the full state space

- Can get convergence very quickly

- Trying to learn the whole state transition matrix
  - Harder to implement for larger ones

- Use the constraints (state transition probabilities)

- $$
  U(i) = R(i) + \sum_{j} M_{ij} U(j)
  $$

  - Solve using DP
  - No maximisation over actions since agent is passive, unlike in value iteration

## Temporal difference learning (TD)

- Only learn local; neighbourhoods;

- $$
  U(i) \leftarrow U(i) + \alpha [R(i) + U(j) - U(i)]
  $$

- Learning parameter $\alpha$
  - Encourages exploration vs exploitation
  - Encourages going to new states, than revisiting already visited states

- A model-free approach

- Fixed policy

- Convergence

  - Average values of $U(i)$ converge to the correct value
  - Decreasing $\alpha$, using a factor of $[N[i]]$, the number of times a state is visited

- $$
  \pi(s) = \arg \max_{a} Q^*(s, a)
  \\
  Q^*(s, a) = \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma U^*(s')]
  $$

## Comparisons

- Each iteration is costlier for ADP, lesser for TD
- ~TD needs more training samples

| Feature                  | Monte-Carlo | Adaptive DP           | TD Learning           |
|--------------------------|-----------------------|-----------------------|-----------------------|
| Implementation           | Simple               | Hard                  | Simple               |
| Update complexity       | Fast                 | Expensive (full eval) | Fast                 |
| Bellman constraints      | Not exploited    | Fully exploited       | Partially exploited  |
| Convergence speed       | Slow                 | Fast (w.r.t. updates) | Medium               |

## Limitations

- ~Local state transition matrix

---

