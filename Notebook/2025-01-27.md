# MPAS | 2025-01-27

## Active reinforcement learning

- In contrast, passive reinforcement learning has a fixed policy

- An active agent explores the world, and requires outcome probabilities of all actions rather than for a fixed policy

- Exploration vs exploitation dilemma
  - Sticking to known world ensures stability but may be sub-optimal
  - 'Curiosity killed the cat'

- Different initializations of rewards for non-terminal states leading to different (optimal) policies, in passive reinforcement learning.
  Agent learning the model and not the true environment.

- Exploration function $f(u, n)$

  - Increasing in $u$ and decreasing in $n$
  - $N(a, s) \to$ Number of times action $a$ is executed in state $s$
  - Greed is traded off with curiosity

    $$
    f(u, n) = \begin{cases}
    R^+, & \text{if } n < N_e \\
    u, & \text{otherwise}
    \end{cases}
    $$

  - Where
    - $R^+$ is an optimistic estimate of the best possible reward obtainable in any state
    - $N_e$ is a fixed parameter
  - This ensures that each state-action pair is tried at least $N_e$ times
- Problems with TD learning
  - TD learns utility values in a local neighborhood instead of whole state space

### $Q$-learning

- Instead, introducting a $Q$-function

  $$
  \pi(s) = \arg \max_{a} Q^*(s, a)
  $$

  $$
  Q^*(s, a) = \sum_{s'} T(s, a, s') \Big[ R(s, a, s') + \gamma^* \, U(s') \Big]
  $$

- Makes action selection model free

- Relationship with utility

  $$
  U(s) = \max_{a} Q(s, a)
  $$

  - Just tells the quality of an action
  - Best action in a state $s$

- Contraint equation for equilibrium

  $$
  Q(s, a) = R(s) + \gamma \sum_{s'} T(s, a, s') \max_{a'} Q(s', a')
  $$

- TD update for utility

  $$
  U^\pi(s) \longleftarrow U^\pi(s) + \alpha \left( R(s) + \gamma \, U^\pi(s') - U^\pi(s) \right)
  $$

- $Q$-learning with TD udpate

  $$
  Q(s, a) \longleftarrow Q(s, a) + \alpha \left( R(s) + \gamma \, \max_{a'} Q(s', a') - Q(s, a) \right)
  $$

- Update after each transition

  $$
  Q^{\text{new}}(S_{t}, A_{t}) \longleftarrow (1 - \alpha) \ \underbrace{ Q(S_{t}, A_{t}) }_{\text{current value}} + \alpha \bigg( \underbrace{ \underbrace{ R_{t+1} }_{\text{reward}} + \gamma \underbrace{ \max_{a} Q(S_{t+1}, a) }_{\text{estimate of optimal future value}} }_{\text{new value (temporal difference target)}} \bigg)
  $$

  where $\alpha$ is the learning rate parameter, and $\gamma$ is the discount factor

### Optimal exploration

- **GLIE** $\to$ Greedy in the limit of infinite exploration
  - Try each action in each state an unbounded number of times
- **Gittins index**
  - <https://en.wikipedia.org/wiki/Gittins_index>

## Multi-armed bandits

### $k$-armed bandit problem

- Gambling context
- All draws are independent given probabilities of reward $\mu_a$ of each arm

### Exploration vs exploitation dilemma

- ~ Exploration is to pull an arm not pulled before
- Exploitation is to choose an arm for which we currently have the highest estimate of $\mu_a$

### Stochastic $k$-armed bandit problem

- Game is played for $T$ rounds
- $\mu_a$'s are fixed but unknown

### Epsilon greedy policy

- Explore with probability $\varepsilon$ and follow greedy strategy with probability $1 - \varepsilon$

- For a suitable choice of $\varepsilon_t$, it holds that $R_T = \mathcal{O}(k \log T) \implies \frac{R_T}{T} \to 0$

- Exploration makes sub-optimal choices
  - Alternates: Confidence interval based selection

## Deep $Q$-networks (DQN)

- $Q$-learning with a deep neural network function approximation called the $Q$-network
- Uses epsilon-greedy strategy for action selection

## Inverse reinforcement learning

- Learning reward distribution from collected samples
- Sampled-based approaches

---
