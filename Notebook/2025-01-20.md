# MPAS | 2025-01-20

## Utility function

- Long term reward for state
- Impurity function?
  - Consider the past, present and future rewards
- Markov property
- Is additive

  $$
  U_h([s_0, s_1, \dots]) = R(s_0) + R(s_1) + \dots
  $$

- Problematic with infinite state sequences
  - Set $R_\max, \gamma < 1$
  - Use average reward per step as basis for comparison
- Discounted case

  $$
  U_h([s_0, s_1, \dots]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots, \qquad 0 \leq \gamma \leq 1
  $$

## Value iteration

- <http://incompleteideas.net/book/ebook/node44.html>
- MEU principle

  $$
  \pi^*(s) = \arg \max_{a} \sum_{s'} T(s, a, s') \ U(s)
  $$

- Bellman equation

  $$
  U(s) = R(s) + \gamma \max_{a} \sum_{s'} T(s, a, s') \ U(s')
  $$

- Bellman update

  $$
  U_{i+1}(s) = R(s) + \gamma \max_{a} \sum_{s'} T(s, a, s') \ U_i(s')
  $$

- Loop till convergence; Convergence guaranteed; Unique solution guaranteed

## Policy iteration

- <http://incompleteideas.net/book/ebook/node43.html>
- <https://gibberblot.github.io/rl-notes/single-agent/policy-iteration.html>
- Initialise a policy instead
- **Policy evaluation**
  - Given a policy $\pi_i$, calculate the utility of each state
- **Policy improvement**
  - Use one-step look ahead to calculate a better policy $\pi_{i+1}$
- Bellman update

  $$
  U_{i+1}(s) = R(s) + \gamma \max_{a} \sum_{s'} T(s, a, s') \ U_i(s')
  $$

  $$
  U_{i+1}(s) = R(s) + \gamma \sum_{s'} T(s, \pi_i(s), s') \ U_i(s')
  $$

- Linear equations in policy iteration

  $$
  U(i) = R(i) + \sum_{j} M_{ij}^{P(i)} U_t(j)
  $$

  - Model $M_{ij]}^{a} = P(j \mid i, a) \rightarrow$ Probability that action $a$ on state $i$ leads to state $j$
  - Use a linear systems solver to solve $n$ equations in $n$ unknowns
- Convergence guaranteed
  - ~A finite MDP only has a finite number of policies
  - ~Monotonically increasing
- Modified policy iteration
  - Run $k$ updates for an estimation of the utilities

---

- Two algorithms, ~Reinforcement learning
  - Adaptive dynamic programming
    - Don't know the state transition probabilities
    - Learn the state transition probabilities
    - Faster convergence
    - Solved for entire state space
  - Temporal difference learning
    - Slower convergence, More iterations required
    - Solved only for subet of the state space $\implies$ Local

## Case studies

- Cognitive model, Cambridge, 2008
  - User space, Device space, modelled as two Markov processes
  - User learning capability, based on feedback from software
  - ? Was it early work, cause it seems granted/ rudimentary now-a-days
- Gaze controlled system
  - Navigation of a robotic manipulator
  - Uncertainty modelling, using Markov decision processes
  - MDP demo

---

