# MPAS | 2025-01-20

## Utility function

- Long term reward for state
- Impurity function?
  - Consider the past, present and future rewards
- Markov property

## Value iteration

- <http://incompleteideas.net/book/ebook/node44.html>

- MEU principle

  - l=l
  
  - $$
    \pi^*(s) = \arg \max_{a}
    $$
  
- Bellman

  - Bellman equation

  - Bellman update

    - $$
      U_{i+1}(s) = R(s) + \gamma \max_{a} \sum_{s'} T(s, a, s') U_i(s')
      $$

- Loop till convergence; Convergence guaranteed

## Policy iteration

- <http://incompleteideas.net/book/ebook/node43.html>

- <https://gibberblot.github.io/rl-notes/single-agent/policy-iteration.html>

- Initialise a policy instead

- Bellman update

  - l=l

  - $$
    U_{i+1}(s) = R(s) + \gamma \max_{a} \sum_{s'} T(s, \pi_i(s), s') U_i(s')
    $$

- Linear equations in policy iteration

  - $$
    U(i) = R(i) + \sum_{j} M_{ij}^{P(i)} U_t(j)
    $$

  - Model $M_{ij]}^{a} = P(j \mid i, a) \rightarrow$ Probability that action $a$ on state $i$ leads to state $j$

  - Use a linear systems solver to solve $n$ equations in $n$ unknowns?

- Convergence guaranteed

  - ~A finite MDP only has a finite number of policies

  - ~Monotonically increasing

- Two algorithms, ~Reinforcement learning

  - Adaptive dynamic program
    - Don't know the state transition probailites
    - Learn the state transition probailites
    - Faster convergence
    - Solved for entire state space
  - Temporal difference learning
    - Slower convergence, More iterations required
    - Solved only for subet of the state space $\implies$ Local

## Case studies

- Cognitive model, Cambridge, 2008
  - User space, Device space, modelled as two Markov processes
  - User learning capability, based on feedback from software
  - ? Was it early work, cause it seems granted/ rudimentary now-a-days

- Gaze controlled system
  - MDP demo

---

